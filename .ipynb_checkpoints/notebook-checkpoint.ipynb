{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This code replicates the model discussed in the following research\n",
    "> A. Chadha, R. Dara and Z. Poljak, \"Convolutional Classification of Pathogenicity in H5 Avian Influenza Strains,\" 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), Boca Raton, FL, USA, 2019, pp. 1570-1577.\n",
    "\n",
    "- Within research 1202 HP sequences, 1167 LP sequences were used which were gathered from various sources such as https://www.fludb.org.  \n",
    "\n",
    "- This code has yet to collect relevant number of data and works with only 133 HP sequences and 750 LP sequences  \n",
    "- These sequences were collected from https://www.fludb.org only  \n",
    "- They are all HA segments of H5 avian influenza virus of various kinds.  \n",
    "- These HA segments are aligned using MUSCLE (Multiple Sequence Comparison by Log-Expectation) algorithm, available [here](https://www.fludb.org/brc/msa.spg?method=ShowCleanInputPage&decorator=influenza)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential, utils\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly Pathogenic cases: 938\n",
      "Low Pathogenic cases: 862\n"
     ]
    }
   ],
   "source": [
    "# Load data in records, and outputs in y\n",
    "data = pd.read_csv(\"./H5.csv\")\n",
    "data = data.iloc[:2459]\n",
    "records = data[\"HA\"].apply(lambda x: np.array(list(x))).values\n",
    "y = data[\"Pathogenicity\"].values\n",
    "\n",
    "print('Highly Pathogenic cases:', np.count_nonzero(y==\"HP\"))\n",
    "print('Low Pathogenic cases:', np.count_nonzero(y==\"LP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(samples, proteins, one-hot-encoding) :: (1800, 589, 21)\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]] tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]], shape=(1800, 2), dtype=float32)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "1350\n",
      "1350\n",
      "450\n",
      "450\n",
      "1800 1800\n"
     ]
    }
   ],
   "source": [
    "# preprocess input data\n",
    "sequence = records\n",
    "le_1 = LabelEncoder()\n",
    "y = le_1.fit_transform(y) \n",
    "y = tf.one_hot(y, 2)\n",
    "le = LabelEncoder()\n",
    "seqEncoded = np.zeros((len(records),len(records[0])),dtype=int)\n",
    "for i, seq in enumerate(sequence):\n",
    "    seqEncoded[i] = le.fit_transform(seq)\n",
    "\n",
    "oneHotSeq = tf.one_hot(seqEncoded, depth=21).numpy()\n",
    "\n",
    "# remove alignment character's one hot\n",
    "for seq in oneHotSeq:\n",
    "    for protein in seq:\n",
    "        if protein[0] == 1:\n",
    "            protein[0] = 0\n",
    "print('(samples, proteins, one-hot-encoding) ::',oneHotSeq.shape)\n",
    "print(oneHotSeq[:1],y)\n",
    "trainTestValues=[]\n",
    "noOfEntries=oneHotSeq.shape[0]\n",
    "for i in range(noOfEntries):\n",
    "  itemArray=[oneHotSeq[i],y[i]]\n",
    "  trainTestValues.append(itemArray)\n",
    "shuffle(trainTestValues)\n",
    "testSplitIndex=int(noOfEntries*0.75)\n",
    "train=trainTestValues[:testSplitIndex]\n",
    "test=trainTestValues[testSplitIndex:]\n",
    "X_train=[i[0] for i in train]\n",
    "X_train=np.array(X_train)\n",
    "print(X_train[0])\n",
    "y_train=[i[1] for i in train]\n",
    "X_test=[i[0] for i in test]\n",
    "y_test=[i[1] for i in test]\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "print(len(X_train)+len(X_test),noOfEntries)\n",
    "#X_train,y_train,X_test,y_test = train_test_split(oneHotSeq,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 294, 20)           860       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 147, 20)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2940)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 5882      \n",
      "=================================================================\n",
      "Total params: 6,742\n",
      "Trainable params: 6,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(20,\n",
    "                 kernel_size=2,\n",
    "                 strides=2,\n",
    "                 activation='relu',\n",
    "                 input_shape=(oneHotSeq.shape[1], oneHotSeq.shape[2],)))\n",
    "model.add(MaxPool1D(pool_size=2,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"optimizer = RMSprop(learning_rate=0.001, rho=0.9)\\nmodel.compile(loss='binary_crossentropy',\\n              optimizer=optimizer,\\n              metrics=['accuracy'])\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compiling model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# compiling model\n",
    "'''optimizer = RMSprop(learning_rate=0.001, rho=0.9)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 1080 samples, validate on 270 samples\n",
      "Epoch 1/13\n",
      "1080/1080 [==============================] - 0s 178us/sample - loss: 0.1429 - accuracy: 0.9620 - val_loss: 0.1608 - val_accuracy: 0.9444\n",
      "Epoch 2/13\n",
      "1080/1080 [==============================] - 0s 166us/sample - loss: 0.1298 - accuracy: 0.9574 - val_loss: 0.1734 - val_accuracy: 0.9704\n",
      "Epoch 3/13\n",
      "1080/1080 [==============================] - 0s 159us/sample - loss: 0.1173 - accuracy: 0.9630 - val_loss: 0.1442 - val_accuracy: 0.9667\n",
      "Epoch 4/13\n",
      "1080/1080 [==============================] - 0s 205us/sample - loss: 0.1106 - accuracy: 0.9620 - val_loss: 0.1456 - val_accuracy: 0.9704\n",
      "Epoch 5/13\n",
      "1080/1080 [==============================] - 0s 217us/sample - loss: 0.1026 - accuracy: 0.9667 - val_loss: 0.1274 - val_accuracy: 0.9704\n",
      "Epoch 6/13\n",
      "1080/1080 [==============================] - 0s 172us/sample - loss: 0.0979 - accuracy: 0.9806 - val_loss: 0.1263 - val_accuracy: 0.9778\n",
      "Epoch 7/13\n",
      "1080/1080 [==============================] - 0s 159us/sample - loss: 0.0916 - accuracy: 0.9667 - val_loss: 0.1195 - val_accuracy: 0.9778\n",
      "Epoch 8/13\n",
      "1080/1080 [==============================] - 0s 174us/sample - loss: 0.0850 - accuracy: 0.9815 - val_loss: 0.1081 - val_accuracy: 0.9778\n",
      "Epoch 9/13\n",
      "1080/1080 [==============================] - 0s 207us/sample - loss: 0.0805 - accuracy: 0.9815 - val_loss: 0.1025 - val_accuracy: 0.9815\n",
      "Epoch 10/13\n",
      "1080/1080 [==============================] - 0s 191us/sample - loss: 0.0738 - accuracy: 0.9796 - val_loss: 0.0950 - val_accuracy: 0.9815\n",
      "Epoch 11/13\n",
      "1080/1080 [==============================] - 0s 157us/sample - loss: 0.0694 - accuracy: 0.9880 - val_loss: 0.0882 - val_accuracy: 0.9815\n",
      "Epoch 12/13\n",
      "1080/1080 [==============================] - 0s 157us/sample - loss: 0.0668 - accuracy: 0.9824 - val_loss: 0.0835 - val_accuracy: 0.9815\n",
      "Epoch 13/13\n",
      "1080/1080 [==============================] - 0s 155us/sample - loss: 0.0615 - accuracy: 0.9852 - val_loss: 0.0771 - val_accuracy: 0.9852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22daa41fa88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model\n",
    "# Due to lack of data model is underfitting (training loss >> validation loss)\n",
    "class_weights = {0: 1.,\n",
    "                 1: 5}\n",
    "'''for st in range(100):\n",
    "  class_weight[1]=2.5+st*0.01\n",
    "  print(class_weight)\n",
    "  model.fit(\n",
    "    np.array(X_train), np.array(y),\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    validation_split=0.3,\n",
    "    shuffle=True,\n",
    "    class_weight=class_weight\n",
    "    )'''\n",
    "model.fit(\n",
    "  np.array(X_train), np.array(y_train),\n",
    "  batch_size=100,\n",
    "  epochs=13,\n",
    "  validation_split=0.2,\n",
    "  shuffle=True,\n",
    "  class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       223\n",
      "           1       0.98      0.99      0.98       227\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       450\n",
      "   macro avg       0.98      0.98      0.98       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      " samples avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(np.array(X_test))\n",
    "y_pred=y_pred>0.5\n",
    "print(classification_report(np.array(y_test),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
