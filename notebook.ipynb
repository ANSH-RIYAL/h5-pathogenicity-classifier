{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This code replicates the model discussed in the following research\n",
    "> A. Chadha, R. Dara and Z. Poljak, \"Convolutional Classification of Pathogenicity in H5 Avian Influenza Strains,\" 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), Boca Raton, FL, USA, 2019, pp. 1570-1577.\n",
    "\n",
    "- Within research 1202 HP sequences, 1167 LP sequences were used which were gathered from various sources such as https://www.fludb.org.  \n",
    "\n",
    "- This code has yet to collect relevant number of data and works with only 133 HP sequences and 750 LP sequences  \n",
    "- These sequences were collected from https://www.fludb.org only  \n",
    "- They are all HA segments of H5 avian influenza virus of various kinds.  \n",
    "- These HA segments are aligned using MUSCLE (Multiple Sequence Comparison by Log-Expectation) algorithm, available [here](https://www.fludb.org/brc/msa.spg?method=ShowCleanInputPage&decorator=influenza)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Bio import SeqIO\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly Pathogenic cases: 133\n",
      "Low Pathogenic cases: 750\n"
     ]
    }
   ],
   "source": [
    "# Load data in records, and outputs in y\n",
    "records = []\n",
    "y = []\n",
    "with open('./all-aligned-seq.fasta') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fasta'):\n",
    "        attr = record.id.split('|')\n",
    "        records.append(record)\n",
    "        if(attr[3] == 'Yes'):\n",
    "            y.append(1)\n",
    "        elif(attr[3] == 'No'):\n",
    "            y.append(0)\n",
    "print('Highly Pathogenic cases:', y.count(1))\n",
    "print('Low Pathogenic cases:', y.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(samples, proteins, one-hot-encoding) :: (883, 576, 21)\n"
     ]
    }
   ],
   "source": [
    "# preprocess input data\n",
    "sequence = np.array(records)\n",
    "y = tf.one_hot(np.array(y), 2)\n",
    "le = LabelEncoder()\n",
    "seqEncoded = np.empty_like(sequence, dtype='int')\n",
    "for i, seq in enumerate(sequence):\n",
    "    seqEncoded[i] = le.fit_transform(seq)\n",
    "\n",
    "oneHotSeq = tf.one_hot(seqEncoded, depth=21).numpy()\n",
    "\n",
    "# remove alignment character's one hot\n",
    "for seq in oneHotSeq:\n",
    "    for protein in seq:\n",
    "        if protein[0] == 1:\n",
    "            protein[0] = 0\n",
    "print('(samples, proteins, one-hot-encoding) ::',oneHotSeq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 288, 20)           860       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 144, 20)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2880)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 5762      \n",
      "=================================================================\n",
      "Total params: 6,622\n",
      "Trainable params: 6,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(20,\n",
    "                 kernel_size=2,\n",
    "                 strides=2,\n",
    "                 activation='relu',\n",
    "                 input_shape=(oneHotSeq.shape[1], oneHotSeq.shape[2],)))\n",
    "model.add(MaxPool1D(pool_size=2,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 618 samples, validate on 265 samples\n",
      "Epoch 1/10\n",
      "618/618 [==============================] - 0s 151us/sample - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "618/618 [==============================] - 0s 149us/sample - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "618/618 [==============================] - 0s 149us/sample - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "618/618 [==============================] - 0s 143us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "618/618 [==============================] - 0s 150us/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "618/618 [==============================] - 0s 151us/sample - loss: 0.0020 - accuracy: 1.0000 - val_loss: 9.8436e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "618/618 [==============================] - 0s 195us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 9.6242e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "618/618 [==============================] - 0s 170us/sample - loss: 0.0017 - accuracy: 1.0000 - val_loss: 9.1780e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "618/618 [==============================] - 0s 168us/sample - loss: 0.0016 - accuracy: 1.0000 - val_loss: 7.9225e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "618/618 [==============================] - 0s 169us/sample - loss: 0.0014 - accuracy: 1.0000 - val_loss: 7.3965e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb9cc095358>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model\n",
    "model.fit(\n",
    "    oneHotSeq, y,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.3,\n",
    "    shuffle=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
